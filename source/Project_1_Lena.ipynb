{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Classification, weight sharing, auxiliary losses \n",
    "\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a\n",
    "two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an\n",
    "auxiliary loss to help the training of the main objective.\n",
    "It should be implemented with PyTorch only code, in particular without using other external libraries\n",
    "such as scikit-learn or numpy.\n",
    "\n",
    "The goal of this project is to implement a deep network such that, given as input a series of 2 ×14×14\n",
    "tensor, corresponding to pairs of 14 × 14 grayscale images, it predicts for each pair if the first digit is\n",
    "lesser or equal to the second. The training and test set should be 1, 000 pairs each, and the size of the images allows to run experiments rapidly, even in the VM with a single core and no GPU.\n",
    "You can generate the data sets to use with the function generate˙pair˙sets(N) defined in the file\n",
    "dlc˙practical˙prologue.py. This function returns six tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import argparse\n",
    "import os\n",
    "import urllib\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dlc_practical_prologue import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/'):\n",
    "    os.makedirs('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once to download the MNIST data-set. \n",
    "# There is a problem with the server on which it's hosted so only way right now \n",
    "# to have it :( \n",
    "'''\n",
    "!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "!tar -zxvf MNIST.tar.gz\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(\n",
    "    1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training and test input size: {train_input.size(), test_input.size()}')\n",
    "print(f'Training and test target size: {train_target.size(), test_target.size()}')\n",
    "print(f'Training and test classes size: {train_classes.size(), test_classes.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset needed for training. For this as we have a special data case we rewrite the `Dataset` class in order to use a `dataloader` later. Remember `target` is 1 if first number is smaller or equal than the second image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, pairs, target, classes):\n",
    "        'Initialization'\n",
    "        # target = (0,1)\n",
    "        self.target = target\n",
    "        # image pairs (2,14,14)\n",
    "        self.pairs = pairs\n",
    "        # cipher classes (2 in [0,9])\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # image pairs\n",
    "        X = self.pairs[index]\n",
    "        # target:\n",
    "        y = self.target[index]\n",
    "        # classes:\n",
    "        Y = self.classes[index]\n",
    "        return X, y, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets (training and validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(train_input, train_target, train_classes)\n",
    "test_set = Dataset(test_input, test_target, test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 2, figsize=(5, 18))\n",
    "for j in range(6):\n",
    "    im1 = training_set.__getitem__(j)[0][0, :, :]\n",
    "    im2 = training_set.__getitem__(j)[0][1, :, :]\n",
    "    target = training_set.__getitem__(j)[1]\n",
    "    classes = training_set.__getitem__(j)[2]\n",
    "    ax[j, 0].imshow(im1, cmap='gray')\n",
    "    ax[j, 1].imshow(im2, cmap='gray')\n",
    "    ax[j, 1].set_title(f'Cipher: {classes[1]}')\n",
    "    ax[j, 0].set_title(f'Cipher: {classes[0]}, target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline: \n",
    "- Loss: CE (cross entropy) \n",
    "- Optimizer: SGD optimizer\n",
    "- Activation function: softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1: \n",
    "Add one more hidden layer: Accuracy: 77.6%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2:\n",
    "Use Sigmoid instead ReLU: Accuracy: 52.6%, Avg loss: 0.010993 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3:\n",
    "Use Tanh instead ReLu: Accuracy: 71.9%, Avg loss: 0.009163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 4:\n",
    "Add Batch Normalization with eps=1e-05, momentum=0.1, affine=True, track_running_stats=True: Accuracy: 75.8%, Avg loss: 0.007833"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas\n",
    "- Dropout\n",
    "- Batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model with two layers and a two digit output:\n",
    "class Model_1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        input_size = 2 * 14 * 14\n",
    "        hidden_sizes = [392, 392]\n",
    "        # two digit output, probability of being 1 or 0:\n",
    "        output_size = 2\n",
    "        # flatten images to 1D input:\n",
    "        self.flatten = nn.Flatten()\n",
    "        # then two hidden layers:\n",
    "        self.model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.BatchNorm1d(num_features=hidden_sizes[0]),\n",
    "                                   nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.BatchNorm1d(num_features=hidden_sizes[1]),\n",
    "                                   nn.Linear(hidden_sizes[1], output_size))\n",
    "        # no need to add softmax at the end because already in CE loss.\n",
    "    def forward(self, x):\n",
    "        # flatten 2D->1D\n",
    "        x = self.flatten(x)\n",
    "        # predict probabilities:\n",
    "        logits = self.model(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader for model, change num_workers when on GPU:\n",
    "params = {'batch_size': 64, 'shuffle': True, 'num_workers': 0}\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "test_generator = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Call model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_1().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training:\n",
    "Binary classification with two output units --> so `CrossEntropyLoss()` so need to use `torch.nn.CrossEntropyLoss` instead of `BCELoss` (BCE for 1 digit output). The `Softmax` activation is already included in this loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss = 0\n",
    "    for batch, (X, y, Y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    # return average training loss:\n",
    "    train_loss /= size\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, Y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # Softmax to get probabilities:\n",
    "            prob = softmax(pred)\n",
    "            # calculate number of correct predictions:\n",
    "            correct += (prob.argmax(1) == y).type(torch.float).sum().item()\n",
    "    # return average test loss and accuracy:\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(\n",
    "        f\"Validation Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "    )\n",
    "    return 100 * correct, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and optimizers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 25\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss, test_loss = [], []\n",
    "accuracy = []\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train_loop(training_generator, model, loss_fn, optimizer)\n",
    "    acc, t_loss = test_loop(test_generator, model, loss_fn)\n",
    "    \n",
    "    accuracy.append(acc)\n",
    "    training_loss.append(train_loss)\n",
    "    test_loss.append(t_loss)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model:\n",
    "torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, '../data/Lena/lena_ex_4.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axs[0].plot(accuracy)\n",
    "axs[0].plot(accuracy)\n",
    "axs[0].set_xlabel('Num epochs')\n",
    "axs[0].set_title('Accuracy')\n",
    "axs[1].plot(test_loss, label=' test_loss')\n",
    "axs[1].plot(training_loss, label='train_loss')\n",
    "axs[1].set_xlabel('Num epochs')\n",
    "axs[1].set_title('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a few predictions:\n",
    "size = len(test_generator.dataset)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "fig, ax = plt.subplots(6, 2, figsize=(5, 18))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (X, y, Y) in enumerate(test_generator):\n",
    "        if batch == 0:\n",
    "            pred = model(X)\n",
    "            prob = softmax(pred)\n",
    "            prediction = prob.argmax(1).type(torch.float)\n",
    "            for j in range(6):\n",
    "                im1 = X[j][0, :, :]\n",
    "                im2 = X[j][1, :, :]\n",
    "                target = y[j]\n",
    "                classes = Y[j]\n",
    "                pred = prediction[j]\n",
    "                ax[j, 0].imshow(im1, cmap='gray')\n",
    "                ax[j, 1].imshow(im2, cmap='gray')\n",
    "                ax[j, 0].set_title(\n",
    "                    f'Cipher: {classes[0]}, target: {target}, pred: {pred}')\n",
    "                ax[j, 1].set_title(f'Cipher: {classes[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
