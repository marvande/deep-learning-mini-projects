{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty\n",
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The module and sgd optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.params = {}\n",
    "        self.w = empty(out_features, in_features).fill_(1)\n",
    "        self.params['weight'] = [self.w]\n",
    "        self.gradwrt_w = []\n",
    "        self.params['grad'] = [self.gradwrt_w]\n",
    "        if bias:\n",
    "            self.b = empty(out_features).fill_(1)\n",
    "            self.params['weight'].append(self.b)\n",
    "            self.gradwrt_b = []\n",
    "            self.params['grad'].append(self.gradwrt_b)\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        self.input = input\n",
    "        l = []\n",
    "        \n",
    "        if self.bias:\n",
    "            l = [self.w @ tensor + self.b for tensor in input]\n",
    "        else:\n",
    "            l = [self.w @ tensor for tensor in input]\n",
    "        \n",
    "        return tuple(l)\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        l = []\n",
    "        \n",
    "        for i in range(len(gradwrtoutput)):\n",
    "            # with respect to the input\n",
    "            l += [(gradwrtoutput[i] @ (self.w))]\n",
    "            \n",
    "            # with respect to the weight\n",
    "            gradwrt_w = gradwrtoutput[i].view(-1,1).mm(self.input[i].view(1,-1))\n",
    "            if len(self.gradwrt_w) != len(gradwrtoutput):\n",
    "                self.gradwrt_w.append(empty(gradwrt_w.size()).fill_(0).squeeze()) # (1) this can be optimized\n",
    "            self.gradwrt_w[i].add_(gradwrt_w.squeeze())\n",
    "         \n",
    "            # with respect to the bias\n",
    "            if self.bias:\n",
    "                gradwrt_b = gradwrtoutput[i]\n",
    "                if len(self.gradwrt_b) != len(gradwrtoutput):\n",
    "                    self.gradwrt_b.append(empty(gradwrt_b.size()).fill_(0).squeeze()) # (1) this can be optimized\n",
    "                self.gradwrt_b[i].add_(gradwrt_b.squeeze())\n",
    "        \n",
    "        return tuple(l)\n",
    "        \n",
    "    def param(self):\n",
    "        return self.params\n",
    "\n",
    "\n",
    "class SGD(object):\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "    \n",
    "    def step(self):\n",
    "        for i in range(len(self.params['weight'])):\n",
    "            for j in range(len(self.params['grad'][0])):\n",
    "                self.params['weight'][i] -= self.lr * self.params['grad'][i][j]\n",
    "            \n",
    "    def zero_grad(self):\n",
    "        for i in range(len(self.params['weight'])):\n",
    "            self.params['grad'][i].clear() # (1) this can be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Control the randomness\n",
    "# ------------------------------------------------------------------\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# model\n",
    "# ------------------------------------------------------------------\n",
    "m = Linear(2, 3)\n",
    "m2 = Linear(3, 4)\n",
    "sgd = SGD(m.param(), 0.1)\n",
    "sgd2 = SGD(m2.param(), 0.1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# input and error and reset the gradients\n",
    "# ------------------------------------------------------------------\n",
    "input = empty(2)\n",
    "input[0] = 1\n",
    "input[1] = 2\n",
    "\n",
    "# arbitrary error need 3 grad_loss because we have 3 inputs below\n",
    "grad_loss = empty(4).fill_(10),  empty(4).fill_(5),  empty(4).fill_(1)\n",
    "\n",
    "# zeroes the gradients as one would do in a training setting\n",
    "sgd.zero_grad()\n",
    "sgd2.zero_grad()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# forward pass\n",
    "# ------------------------------------------------------------------\n",
    "x = m.forward(input, input, input)\n",
    "x = m2.forward(*x)\n",
    "print(\"m params after forward {}\\n\".format(m.param()))\n",
    "print(\"m2 params after forward {}\\n\".format(m2.param()))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# backward pass\n",
    "# ------------------------------------------------------------------\n",
    "x = m2.backward(*grad_loss)\n",
    "output = m.backward(*x)\n",
    "print(\"output (error with respect to input) {}\\n\".format(output))\n",
    "print(\"m params after backward {}\\n\".format(m.param()))\n",
    "print(\"m2 params after backward {}\\n\".format(m2.param()))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# gradient step\n",
    "# ------------------------------------------------------------------\n",
    "sgd.step()\n",
    "sgd2.step()\n",
    "print(\"m params after step {}\\n\".format(m.param()))\n",
    "print(\"m2 params after step {}\\n\".format(m2.param()))\n",
    "\n",
    "# zeroes the gradients as one would do in a training setting\n",
    "sgd.zero_grad()\n",
    "sgd2.zero_grad()\n",
    "print(\"m params after zeroing the gradients {}\\n\".format(m.param()))\n",
    "print(\"m2 params after zeroing the gradients {}\\n\".format(m2.param()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
