{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "from relu import ReLU\n",
    "from sgd import SGD, SGD_Sequential\n",
    "from leakyrelu import LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The module and sgd optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "\n",
    "\n",
    "class Linear(Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.params = {}\n",
    "        self.w = empty(out_features, in_features).fill_(1)\n",
    "        self.params['weight'] = [self.w]\n",
    "        self.gradwrt_w = []\n",
    "        self.params['grad'] = [self.gradwrt_w]\n",
    "        if bias:\n",
    "            self.b = empty(out_features).fill_(1)\n",
    "            self.params['weight'].append(self.b)\n",
    "            self.gradwrt_b = []\n",
    "            self.params['grad'].append(self.gradwrt_b)\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        self.input = input\n",
    "        l = []\n",
    "        \n",
    "        if self.bias:\n",
    "            l = [self.w @ tensor + self.b for tensor in input]\n",
    "        else:\n",
    "            l = [self.w @ tensor for tensor in input]\n",
    "        \n",
    "        return tuple(l)\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        l = []\n",
    "        \n",
    "        for i in range(len(gradwrtoutput)):\n",
    "            # with respect to the input\n",
    "            l += [(gradwrtoutput[i] @ (self.w))]\n",
    "            \n",
    "            # with respect to the weight\n",
    "            gradwrt_w = gradwrtoutput[i].view(-1,1).mm(self.input[i].view(1,-1))\n",
    "            if len(self.gradwrt_w) != len(gradwrtoutput):\n",
    "                self.gradwrt_w.append(empty(gradwrt_w.size()).fill_(0).squeeze()) # (1) this can be optimized\n",
    "            self.gradwrt_w[i].add_(gradwrt_w.squeeze())\n",
    "         \n",
    "            # with respect to the bias\n",
    "            if self.bias:\n",
    "                gradwrt_b = gradwrtoutput[i]\n",
    "                if len(self.gradwrt_b) != len(gradwrtoutput):\n",
    "                    self.gradwrt_b.append(empty(gradwrt_b.size()).fill_(0).squeeze()) # (1) this can be optimized\n",
    "                self.gradwrt_b[i].add_(gradwrt_b.squeeze())\n",
    "        \n",
    "        return tuple(l)\n",
    "        \n",
    "    def param(self):\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    def __init__(self, modules, types):\n",
    "        self.modules = modules\n",
    "        \n",
    "        # reverse order of modules for backward pass:\n",
    "        self.N = len(modules)\n",
    "        self.reverse_modules = [self.modules[(self.N)-i-1] for i in range(self.N)]\n",
    "        self.input = None\n",
    "        self.types = types\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        self.input = input\n",
    "        \n",
    "        # number of modules:\n",
    "        N = len(self.modules)\n",
    "        first_layer = self.modules[0]\n",
    "        print(first_layer.param())\n",
    "        x = first_layer.forward(*self.input)\n",
    "        \n",
    "        for i in range(1,N):\n",
    "            module = self.modules[i]\n",
    "            output_layer_i = module.forward(*x)\n",
    "            # output becomes input to next layer\n",
    "            x = output_layer_i\n",
    "            \n",
    "        return tuple(x)\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        self.input = gradwrtoutput\n",
    "        \n",
    "        # number of modules:\n",
    "        N = len(self.reverse_modules)\n",
    "        \n",
    "        # feed gradient to last layer:\n",
    "        last_layer = self.reverse_modules[0]\n",
    "        x = last_layer.backward(*gradwrtoutput)\n",
    "        \n",
    "        # give to layers N-1 -> 1:\n",
    "        for i in range(1,N):\n",
    "            module = self.reverse_modules[i]\n",
    "            output_layer_i = module.backward(*x)\n",
    "            # output becomes input to next layer\n",
    "            x = output_layer_i\n",
    "        return tuple(x)\n",
    "        \n",
    "    def param(self):\n",
    "        par = {}\n",
    "        for i in range(len(self.types)):\n",
    "            par[self.types[i]] = self.modules[i].param()\n",
    "        #return [module.param() for  module in self.modules]\n",
    "        return par"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Control the randomness\n",
    "# ------------------------------------------------------------------\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# model\n",
    "# ------------------------------------------------------------------\n",
    "m = Linear(2, 3)\n",
    "m2 = Linear(3, 4)\n",
    "\n",
    "\n",
    "\n",
    "# give input as list of modules\n",
    "types = ['linear1', 'linear2']\n",
    "s = Sequential([m, m2], types)\n",
    "\n",
    "\n",
    "sgd = SGD_Sequential(s.param(),types, 0.1)\n",
    "print('# ------------------------------------------------------------------')\n",
    "print('# Set up, weight initialization:')\n",
    "print(\"Params of modules after initialization:\\n\")\n",
    "for i in range(len(s.param())):\n",
    "    print(\"Module {},{}:\\n\".format(i, types[i]))\n",
    "    print(s.param()[types[i]])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# input and error and reset the gradients\n",
    "# ------------------------------------------------------------------\n",
    "input = empty(2)\n",
    "input[0] = 1\n",
    "input[1] = 2\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# forward pass\n",
    "# ------------------------------------------------------------------\n",
    "x = s.forward(input, input, input)\n",
    "print('# ------------------------------------------------------------------')\n",
    "print('# Forward pass:')\n",
    "print(\"Output after forward pass:\\n {}\".format(x))\n",
    "print(\"Params of modules after forward:\\n\")\n",
    "for i in range(len(s.param())):\n",
    "    print(\"Module {},{}:\\n\".format(i, types[i]))\n",
    "    print(s.param()[types[i]])\n",
    "\n",
    "# arbitrary error need 3 grad_loss because we have 3 inputs below\n",
    "grad_loss = empty(4).fill_(10),  empty(4).fill_(5),  empty(4).fill_(1)\n",
    "\n",
    "# zeroes the gradients as one would do in a training setting\n",
    "sgd.zero_grad()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# backward pass\n",
    "# ------------------------------------------------------------------\n",
    "print('# ------------------------------------------------------------------')\n",
    "output = s.backward(*grad_loss)\n",
    "print('# Backward pass:')\n",
    "print(\"Output after backward pass:\\n {}\".format(output))\n",
    "print(\"Params of modules after backward:\\n\")\n",
    "for i in range(len(s.param())):\n",
    "    print(\"Module {},{}:\\n\".format(i, types[i]))\n",
    "    print(s.param()[types[i]])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# gradient step\n",
    "# ------------------------------------------------------------------\n",
    "print('# ------------------------------------------------------------------')\n",
    "print('# Gradient step:')\n",
    "sgd.step()\n",
    "print(\"Params of modules after step:\\n\")\n",
    "for i in range(len(s.param())):\n",
    "    print(\"Module {},{}:\\n\".format(i, types[i]))\n",
    "    print(s.param()[types[i]])\n",
    "\n",
    "\n",
    "# zeroes the gradients as one would do in a training setting\n",
    "sgd.zero_grad()\n",
    "print(\"Params of modules after zeroing the gradients:\\n\")\n",
    "for i in range(len(s.param())):\n",
    "    print(\"Module {},{}:\\n\".format(i, types[i]))\n",
    "    print(s.param()[types[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test with ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def relu(self, x):\n",
    "        z = empty(x.size()).fill_(0)\n",
    "        relu = x.maximum(z)\n",
    "        print(\"your x\", x, \"your z\", z, \"your relu\", relu)\n",
    "        return relu\n",
    "    \n",
    "    def d_relu(self, x):\n",
    "        d_relu = x.apply_(lambda x: 1 if x > 0 else 0)\n",
    "        print(\"your x\", x, \"your d_relu\", d_relu)\n",
    "        return d_relu\n",
    "        \n",
    "    def forward (self, *input):\n",
    "        self.input = input\n",
    "\n",
    "        return tuple([self.relu(tensor) for tensor in input])\n",
    "        \n",
    "    def backward (self, *gradwrtoutput):     \n",
    "        return tuple([gradwrtoutput[i] * self.d_relu(self.input[i]) for i in range(len(self.input))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Control the randomness\n",
    "# ------------------------------------------------------------------\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# model\n",
    "# ------------------------------------------------------------------\n",
    "m = Linear(2, 3)\n",
    "m2 = Linear(3, 4)\n",
    "relu = LeakyReLU(0.1)\n",
    "\n",
    "\n",
    "# give input as list of modules\n",
    "types = ['linear1', 'activation1','linear2']\n",
    "s = Sequential([m, relu, m2],types)\n",
    "\n",
    "sgd = SGD_Sequential(s.param(), types, 0.1)\n",
    "print('# ------------------------------------------------------------------')\n",
    "print('# Set up, weight initialization:')\n",
    "print(\"Params of modules after initialization:\\n\")\n",
    "for i in range(len(s.param())):\n",
    "    print(\"Module {}, {}:\\n\".format(i, types[i]))\n",
    "    print(s.param()[types[i]])\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# input and error and reset the gradients\n",
    "# ------------------------------------------------------------------\n",
    "input = empty(2)\n",
    "input[0] = 1\n",
    "input[1] = 2\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# forward pass\n",
    "# ------------------------------------------------------------------\n",
    "x = s.forward(input, input, input)\n",
    "print('# ------------------------------------------------------------------')\n",
    "print('# Forward pass:')\n",
    "print(\"Output after forward pass:\\n {}\".format(x))\n",
    "print(\"Params of modules after forward:\\n\")\n",
    "for i in range(len(s.param())):\n",
    "    print(\"Module {}, {}:\\n\".format(i, types[i]))\n",
    "    print(s.param()[types[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare to building it by hand without using sequential, like raphael did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# model\n",
    "# ------------------------------------------------------------------\n",
    "m = Linear(2, 3)\n",
    "m2 = Linear(3, 4)\n",
    "\n",
    "\n",
    "sgd = SGD(m.param(), 0.1)\n",
    "sgd2 = SGD(m2.param(), 0.1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# input and error and reset the gradients\n",
    "# ------------------------------------------------------------------\n",
    "input = empty(2)\n",
    "input[0] = 1\n",
    "input[1] = 2\n",
    "\n",
    "# arbitrary error need 3 grad_loss because we have 3 inputs below\n",
    "grad_loss = empty(4).fill_(10),  empty(4).fill_(5),  empty(4).fill_(1)\n",
    "\n",
    "# zeroes the gradients as one would do in a training setting\n",
    "sgd.zero_grad()\n",
    "sgd2.zero_grad()\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# forward pass\n",
    "# ------------------------------------------------------------------\n",
    "x = m.forward(input, input, input)\n",
    "x = m2.forward(*x)\n",
    "print(\"m params after forward {}\\n\".format(m.param()))\n",
    "print(\"m2 params after forward {}\\n\".format(m2.param()))\n",
    "print(\"output after pass trough m and m2 {}\\n\".format(x))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# backward pass\n",
    "# ------------------------------------------------------------------\n",
    "x = m2.backward(*grad_loss)\n",
    "output = m.backward(*x)\n",
    "print(\"output (error with respect to input) {}\\n\".format(output))\n",
    "print(\"m params after backward {}\\n\".format(m.param()))\n",
    "print(\"m2 params after backward {}\\n\".format(m2.param()))\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# gradient step\n",
    "# ------------------------------------------------------------------\n",
    "sgd.step()\n",
    "sgd2.step()\n",
    "print(\"m params after step {}\\n\".format(m.param()))\n",
    "print(\"m2 params after step {}\\n\".format(m2.param()))\n",
    "\n",
    "# zeroes the gradients as one would do in a training setting\n",
    "sgd.zero_grad()\n",
    "sgd2.zero_grad()\n",
    "print(\"m params after zeroing the gradients {}\\n\".format(m.param()))\n",
    "print(\"m2 params after zeroing the gradients {}\\n\".format(m2.param()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
