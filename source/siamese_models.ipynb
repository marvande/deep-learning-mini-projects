{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Classification, weight sharing, auxiliary losses \n",
    "\n",
    "\n",
    "The objective of this project is to test different architectures to compare two digits visible in a\n",
    "two-channel image. It aims at showing in particular the impact of weight sharing, and of the use of an\n",
    "auxiliary loss to help the training of the main objective.\n",
    "It should be implemented with PyTorch only code, in particular without using other external libraries\n",
    "such as scikit-learn or numpy.\n",
    "\n",
    "The goal of this project is to implement a deep network such that, given as input a series of 2 ×14×14\n",
    "tensor, corresponding to pairs of 14 × 14 grayscale images, it predicts for each pair if the first digit is\n",
    "lesser or equal to the second. The training and test set should be 1, 000 pairs each, and the size of the images allows to run experiments rapidly, even in the VM with a single core and no GPU.\n",
    "You can generate the data sets to use with the function generate˙pair˙sets(N) defined in the file\n",
    "dlc˙practical˙prologue.py. This function returns six tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import argparse\n",
    "import os\n",
    "import urllib\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt # to generate nice plots\n",
    "from sklearn.metrics import roc_auc_score # roc auc metric\n",
    "import seaborn as sns # visualization library\n",
    "import pandas as pd\n",
    "from dlc_practical_prologue import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "# !{sys.executable} -m nbstripout --install --global\n",
    "# !{sys.executable} -m pip install scikit-plot\n",
    "# !{sys.executable} -m pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from six.moves import urllib\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('../data/'):\n",
    "    os.makedirs('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once to download the MNIST data-set. \n",
    "# There is a problem with the server on which it's hosted so only way right now \n",
    "# to have it :( \n",
    "\n",
    "# !wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n",
    "# !tar -zxvf MNIST.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = generate_pair_sets(\n",
    "    1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training and test input size: {train_input.size(), test_input.size()}')\n",
    "print(f'Training and test target size: {train_target.size(), test_target.size()}')\n",
    "print(f'Training and test classes size: {train_classes.size(), test_classes.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset needed for training. For this as we have a special data case we rewrite the `Dataset` class in order to use a `dataloader` later. Remember `target` is 1 if first number is smaller or equal than the second image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, pairs, target, classes):\n",
    "        'Initialization'\n",
    "        # target = (0,1)\n",
    "        self.target = target\n",
    "        # image pairs (2,14,14)\n",
    "        self.pairs = pairs\n",
    "        # cipher classes (2 in [0,9])\n",
    "        self.classes = classes\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # image pairs\n",
    "        X = self.pairs[index]\n",
    "        # target:\n",
    "        y = self.target[index]\n",
    "        # classes:\n",
    "        Y = self.classes[index]\n",
    "        return X, y, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets (training and validation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = Dataset(train_input, train_target, train_classes)\n",
    "test_set = Dataset(test_input, test_target, test_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, 2, figsize=(5, 18))\n",
    "for j in range(6):\n",
    "    im1 = training_set.__getitem__(j)[0][0, :, :]\n",
    "    im2 = training_set.__getitem__(j)[0][1, :, :]\n",
    "    target = training_set.__getitem__(j)[1]\n",
    "    classes = training_set.__getitem__(j)[2]\n",
    "    ax[j, 0].imshow(im1, cmap='gray')\n",
    "    ax[j, 1].imshow(im2, cmap='gray')\n",
    "    ax[j, 1].set_title(f'Cipher: {classes[1]}')\n",
    "    ax[j, 0].set_title(f'Cipher: {classes[0]}, target: {target}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, print_loss=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    train_loss, correct = 0, 0\n",
    "    aucs = []\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    \n",
    "    for batch, (X, y, Y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss:\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation:\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        if print_loss:\n",
    "            if batch % 10 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            \n",
    "        # Softmax to get probabilities:\n",
    "        prob = softmax(pred)\n",
    "        \n",
    "        # compute accuracy and roc_auc:\n",
    "        correct += (prob.argmax(1) == y).type(torch.float).sum().item()\n",
    "        aucs.append(roc_auc_score(y.detach().numpy(), prob.detach().numpy()[:, 1]))\n",
    "        \n",
    "    # return average training loss:\n",
    "    train_loss /= size\n",
    "    correct *= 100 / size\n",
    "    auc = sum(aucs)/len(aucs)\n",
    "    \n",
    "    return correct, train_loss, auc\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, print_loss=True):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "    aucs = []\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y, Y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "            # Softmax to get probabilities:\n",
    "            prob = softmax(pred)\n",
    "            \n",
    "            # compute accuracy and roc_auc:\n",
    "            correct += (prob.argmax(1) == y).type(torch.float).sum().item()\n",
    "            aucs.append(roc_auc_score(y.detach().numpy(), prob.detach().numpy()[:, 1]))\n",
    "            \n",
    "    # return average test loss and accuracy:\n",
    "    test_loss /= size\n",
    "    correct *= 100 / size\n",
    "    auc = sum(aucs)/len(aucs)\n",
    "    \n",
    "    if print_loss:\n",
    "        print(\n",
    "            f\"Validation Error: \\n Accuracy: {(correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\"\n",
    "        )\n",
    "        \n",
    "    return correct, test_loss, auc\n",
    "\n",
    "\n",
    "def train_eval(model, optimizer, loss_fn, epochs=25, save=False, print_loss=True, print_epoch=True):\n",
    "    # Data loader for model, change num_workers when on GPU:\n",
    "    params = {'batch_size': 64, 'shuffle': True, 'num_workers': 0}\n",
    "    training_generator = torch.utils.data.DataLoader(training_set, **params)\n",
    "    test_generator = torch.utils.data.DataLoader(test_set, **params)\n",
    "    train_perf, test_perf = [], []\n",
    "\n",
    "    for t in range(epochs):\n",
    "        if print_epoch:\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_perf.append(train_loop(training_generator, model, loss_fn, optimizer, print_loss=print_loss))\n",
    "        test_perf.append(test_loop(test_generator, model, loss_fn, print_loss=print_loss))\n",
    "        \n",
    "    print(\"Done!\")\n",
    "    \n",
    "    if save:\n",
    "        torch.save({'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_perf': train_perf,\n",
    "                    'test_perf': test_perf\n",
    "                    }, '../data/{}'.format(type(model).__name__))\n",
    "        print(\"Saved!\")\n",
    "    \n",
    "    return train_perf, test_perf\n",
    "\n",
    "\n",
    "def plot_performance(train_perf, test_perf):\n",
    "    def sub_plot(axs_id, train_data, test_data, train_label, test_label, x_label, title):\n",
    "        axs[axs_id].plot(train_data, label=train_label)\n",
    "        axs[axs_id].plot(test_data, label=test_label)\n",
    "        axs[axs_id].set_xlabel(x_label)\n",
    "        axs[axs_id].set_title(title)\n",
    "        axs[axs_id].legend()\n",
    "        \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(8, 4))\n",
    "    \n",
    "    train_accs = list(zip(*train_perf))[0]\n",
    "    test_accs = list(zip(*test_perf))[0]\n",
    "    sub_plot(0, train_accs, test_accs, 'train accuracy', 'test accuracy', 'Num epochs', 'Accuracy')\n",
    "    \n",
    "    train_losses = list(zip(*train_perf))[1]\n",
    "    test_losses = list(zip(*test_perf))[1]\n",
    "    sub_plot(1, train_losses, test_losses, 'train loss', 'test loss', 'Num epochs', 'Loss')\n",
    "    \n",
    "    train_aucs = list(zip(*train_perf))[2]\n",
    "    test_aucs = list(zip(*test_perf))[2]\n",
    "    sub_plot(2, train_aucs, test_aucs, 'train auc', 'test auc', 'Num epochs', 'ROC AUC')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "def plot_performance_runs(performance, metrics=['Accuracy', 'Loss', 'ROC_AUC'], style=0):\n",
    "    df = pd.DataFrame(columns=['Run', 'is_Test', 'epoch'] + metrics)\n",
    "    for i in range(len(performance)):\n",
    "        for j in range(2):\n",
    "            df_temp = pd.DataFrame(performance[i][j], columns=['Accuracy', 'Loss', 'ROC_AUC']).reset_index().rename(columns={'index':'epoch'})\n",
    "            df_temp.insert(0, \"is_Test\", j)\n",
    "            df_temp.insert(0, \"Run\", i)\n",
    "            df = df.append(df_temp)\n",
    "            df = df.reset_index().drop(columns=['index'])\n",
    "    \n",
    "    sns.set_theme()\n",
    "    for metric in metrics:\n",
    "        if style == 0:\n",
    "            g = sns.lineplot(data=df, x='epoch', y=metric, hue='is_Test')\n",
    "            g.legend(['Train','Test'])\n",
    "        if style == 1:\n",
    "            g = sns.lineplot(data=df, x='epoch', y=metric, hue='is_Test', style='Run')\n",
    "            g.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.title(\"{} vs {}\".format(metric, \"epoch\"))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese network (with weight sharing): \n",
    "- Type: Siamese\n",
    "- Layers: Fully connected\n",
    "- Shared weights: Yes\n",
    "- Loss: CE (cross entropy) \n",
    "- Activation function: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = 1 * 14 * 14\n",
    "        hidden_sizes = [50, 50]\n",
    "        # we need an intermediate output, because we are using a siamese network\n",
    "        intermediate_output_size = 2\n",
    "        # two digit output, probability of being 1 or 0:\n",
    "        output_size = 2\n",
    "        # flatten images to 1D input:\n",
    "        self.flatten = nn.Flatten()\n",
    "        # then two hidden layers:\n",
    "        self.fc_seq = nn.Sequential(nn.Linear(self.input_size, hidden_sizes[0], bias=False),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_sizes[0], hidden_sizes[1], bias=False),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_sizes[1], intermediate_output_size, bias=False))\n",
    "        self.fcout = nn.Linear(2*intermediate_output_size, output_size)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = x[:, 0].view(-1, self.input_size)\n",
    "        x2 = x[:, 1].view(-1, self.input_size)\n",
    "        output1 = self.fc_seq(x1)\n",
    "        output2 = self.fc_seq(x2)\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "        \n",
    "        # flatten 2D->1D\n",
    "        output = self.flatten(output)\n",
    "        \n",
    "        # predict probabilities:\n",
    "        logits = self.fcout(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese network (without weight sharing): \n",
    "- Type: Siamese\n",
    "- Layers: Fully connected\n",
    "- Shared weights: No\n",
    "- Loss: CE (cross entropy) \n",
    "- Activation function: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNoSharingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_size = 1 * 14 * 14\n",
    "        hidden_sizes = [50, 50]\n",
    "        # we need an intermediate output, because we are using a siamese network\n",
    "        intermediate_output_size = 2\n",
    "        # two digit output, probability of being 1 or 0:\n",
    "        output_size = 2\n",
    "        # flatten images to 1D input:\n",
    "        self.flatten = nn.Flatten()\n",
    "        # then two hidden layers:\n",
    "        self.fc_seq2 = nn.Sequential(nn.Linear(self.input_size, hidden_sizes[0], bias=False),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_sizes[0], hidden_sizes[1], bias=False),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_sizes[1], intermediate_output_size, bias=False))\n",
    "        self.fc_seq1 = nn.Sequential(nn.Linear(self.input_size, hidden_sizes[0], bias=False),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_sizes[0], hidden_sizes[1], bias=False),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.Linear(hidden_sizes[1], intermediate_output_size, bias=False))\n",
    "        self.fcout = nn.Linear(2*intermediate_output_size, output_size)\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = x[:, 0].view(-1, self.input_size)\n",
    "        x2 = x[:, 1].view(-1, self.input_size)\n",
    "        output1 = self.fc_seq1(x1)\n",
    "        output2 = self.fc_seq2(x2)\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "        \n",
    "        # flatten 2D->1D\n",
    "        output = self.flatten(output)\n",
    "        \n",
    "        # predict probabilities:\n",
    "        logits = self.fcout(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese convolutional network (with weight sharing): \n",
    "- Type: Siamese\n",
    "- Layers: Convolutional and Fully connected (at the end)\n",
    "- Shared weights: Yes\n",
    "- Loss: CE (cross entropy) \n",
    "- Activation function: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model with two layers and a two digit output:\n",
    "class SiameseConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.length = 14\n",
    "        self.input_size = 1 * 14 * 14\n",
    "        # we need an intermediate output, because we are using a siamese network\n",
    "        intermediate_output_size = 2\n",
    "        # two digit output, probability of being 1 or 0:\n",
    "        output_size = 2\n",
    "        # flatten images to 1D input:\n",
    "        self.flatten = nn.Flatten()\n",
    "        # convolutional layers\n",
    "        kernel_size = 3\n",
    "        n_channel = 5\n",
    "        self.conv_layer = nn.Sequential(nn.Conv2d(1, n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(n_channel, 2 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(2 * n_channel, 4 * n_channel, kernel_size),\n",
    "                                        nn.BatchNorm2d(4 * n_channel),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout2d(p=0.3),\n",
    "                                        nn.Conv2d(4 * n_channel, 8 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout2d(p=0.4),\n",
    "                                        nn.Conv2d(8 * n_channel, 16 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout2d(p=0.5),\n",
    "                                        nn.Conv2d(16 * n_channel, 32 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Dropout2d(p=0.6)\n",
    "                                        )\n",
    "    \n",
    "        def compute_conv2d_size(length):\n",
    "            return  length - (kernel_size - 1) - 1 + 1\n",
    "        \n",
    "        length_out = self.length\n",
    "        depth = 6\n",
    "        \n",
    "        for i in range(depth):\n",
    "            length_out = compute_conv2d_size(length_out)\n",
    "        \n",
    "        concat_size = length_out * length_out * 2**depth * n_channel\n",
    "#         self.fc = nn.Linear(concat_size, 20)\n",
    "        self.fcout = nn.Linear(concat_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, 0].view(-1, 1, self.length, self.length)\n",
    "        x2 = x[:, 1].view(-1, 1, self.length, self.length)\n",
    "        output1 = self.conv_layer(x1)\n",
    "        output2 = self.conv_layer(x2)\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "        # flatten 2D->1D\n",
    "        output = self.flatten(output)\n",
    "#         output = self.fc(output)\n",
    "        # predict probabilities:\n",
    "        logits = self.fcout(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Siamese convolutional network (without weight sharing): \n",
    "- Type: Siamese\n",
    "- Layers: Convolutional and Fully connected (at the end)\n",
    "- Shared weights: No\n",
    "- Loss: CE (cross entropy) \n",
    "- Activation function: softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic model with two layers and a two digit output:\n",
    "class SiameseConvNoSharingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()       \n",
    "        self.length = 14\n",
    "        self.input_size = 1 * 14 * 14\n",
    "        # we need an intermediate output, because we are using a siamese network\n",
    "        intermediate_output_size = 2\n",
    "        # two digit output, probability of being 1 or 0:\n",
    "        output_size = 2\n",
    "        # flatten images to 1D input:\n",
    "        self.flatten = nn.Flatten()\n",
    "        # convolutional layers\n",
    "        kernel_size = 3\n",
    "        n_channel = 3\n",
    "        self.conv_layer1 = nn.Sequential(nn.Conv2d(1, n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(n_channel, 2 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(2 * n_channel, 4 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(4 * n_channel, 8 * n_channel, kernel_size),\n",
    "                                        nn.ReLU()\n",
    "                                        )\n",
    "        self.conv_layer2 = nn.Sequential(nn.Conv2d(1, n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(n_channel, 2 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(2 * n_channel, 4 * n_channel, kernel_size),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Conv2d(4 * n_channel, 8 * n_channel, kernel_size),\n",
    "                                        nn.ReLU()\n",
    "                                        )\n",
    "    \n",
    "        def compute_conv2d_size(length):\n",
    "            return  length - (kernel_size - 1) - 1 + 1\n",
    "        \n",
    "        length_out = self.length\n",
    "        depth = 4\n",
    "        \n",
    "        for i in range(depth):\n",
    "            length_out = compute_conv2d_size(length_out)\n",
    "        \n",
    "        concat_size = length_out * length_out * 2**depth * n_channel\n",
    "        self.fcout = nn.Linear(concat_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, 0].view(-1, 1, self.length, self.length)\n",
    "        x2 = x[:, 1].view(-1, 1, self.length, self.length)\n",
    "        output1 = self.conv_layer1(x1)\n",
    "        output2 = self.conv_layer2(x2)\n",
    "        output = torch.cat((output1, output2), 1)\n",
    "        # flatten 2D->1D\n",
    "        output = self.flatten(output)\n",
    "        # predict probabilities:\n",
    "        logits = self.fcout(output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# Control the randomness\n",
    "# ------------------------------------------------------------------\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Perform n_runs runs\n",
    "# ------------------------------------------------------------------\n",
    "perf = []\n",
    "n_runs = 10\n",
    "\n",
    "for i in range(n_runs):\n",
    "### ------------------------------------------------------------------\n",
    "### Initialize/reset the model and optimizer (i.e. reset the weights)\n",
    "### ------------------------------------------------------------------\n",
    "    model = SiameseConvNet().to(device)\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "### ------------------------------------------------------------------\n",
    "### Train and evaluate\n",
    "### ------------------------------------------------------------------\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    perf += [train_eval(model, optimizer, loss_fn=loss_fn, print_loss=False, print_epoch=False)]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Plot performance\n",
    "# ------------------------------------------------------------------\n",
    "plot_performance_runs(perf, ['Loss', 'Accuracy', 'ROC_AUC'])\n",
    "\n",
    "# roc curves\n",
    "y_probas = nn.functional.softmax(model(test_input), dim=1).detach().numpy()\n",
    "y_test = test_target\n",
    "skplt.metrics.plot_roc(y_test, y_probas)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions on test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a few predictions:\n",
    "# size = len(test_generator.dataset)\n",
    "# softmax = torch.nn.Softmax(dim=1)\n",
    "# fig, ax = plt.subplots(6, 2, figsize=(5, 18))\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch, (X, y, Y) in enumerate(test_generator):\n",
    "#         if batch == 0:\n",
    "#             pred = model(X)\n",
    "#             prob = softmax(pred)\n",
    "#             prediction = prob.argmax(1).type(torch.float)\n",
    "#             for j in range(6):\n",
    "#                 im1 = X[j][0, :, :]\n",
    "#                 im2 = X[j][1, :, :]\n",
    "#                 target = y[j]\n",
    "#                 classes = Y[j]\n",
    "#                 pred = prediction[j]\n",
    "#                 ax[j, 0].imshow(im1, cmap='gray')\n",
    "#                 ax[j, 1].imshow(im2, cmap='gray')\n",
    "#                 ax[j, 0].set_title(\n",
    "#                     f'Cipher: {classes[0]}, target: {target}, pred: {pred}')\n",
    "#                 ax[j, 1].set_title(f'Cipher: {classes[1]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
